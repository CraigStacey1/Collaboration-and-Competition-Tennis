{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Continuous Control Project\n",
    "\n",
    "## Environment description\n",
    "The output below is from the unity environment initialisation\n",
    "\n",
    "Unity brain name: TennisBrain\n",
    "        Number of Visual Observations (per agent): 0\n",
    "        Vector Observation space type: continuous\n",
    "        Vector Observation space size (per agent): 8\n",
    "        Number of stacked Vector Observation: 3\n",
    "        Vector Action space type: continuous\n",
    "        Vector Action space size (per agent): 2\n",
    "        Vector Action descriptions: , \n",
    "Size of each action: 4\n",
    "\n",
    "There are 2 agents. Each observes a state with length: 8\n",
    "        \n",
    "This shows there are 8 State Space, and 4 Action spaces for the environemnt.\n",
    "\n",
    "### Solving the Environment\n",
    "\n",
    "The Environment is considered solved when an average score of +0.5 or more is achieved over 100 episodes.\n",
    "        \n",
    "## Model Description\n",
    "This project uses PyTorch to construct a neural network for each of the Actor and Critic portions of the agent:\n",
    "\n",
    "The Actor Has:\n",
    "    - 8 inputs (State size from environment) \n",
    "    - 256 First fully connected hidden layer \n",
    "    - 128 Second Fully connected hidden layer \n",
    "    - 2 output (Action size from environment)\n",
    "    \n",
    "The Critic has:\n",
    "    - 8 inputs (State size from environment) \n",
    "    - 256 First fully connected hidden layer \n",
    "    - 128 Second Fully connected hidden layer \n",
    "    - 1 output (Value for the (State, Action) pair)\n",
    "     \n",
    "The hidden layers all use the Relu activation function.\n",
    "\n",
    "## Agent Description\n",
    "The algorithm used for the solution is a DDPG agent, this agent uses 4 neural networks in order to learn the task. \n",
    "    - Actor Local\n",
    "    - Actor Target\n",
    "    - Critic Local\n",
    "    - Critic Target\n",
    "\n",
    "### Q-Network\n",
    "The goal of a Q Network is to learn the optimal policy to solve a given task. This is achieved by initially performing random action given the current state and using the reward from the environment to update the likelyhood of choosing that response again. Over time the Q-Network will determine the best action to perform given a state from the environment. When a policy that performes the best action for each state has been found then the optimal policy has been reached.\n",
    "\n",
    "### DQN\n",
    "A DQN is a reinforcement learning Algorithm that uses a neural network for the function approximator. In this instance the model described above is used at the centre of the algorithm to learn the required actions to perform for a given environment state.\n",
    "\n",
    "### DDPG\n",
    "The DDPG agent uses a similar methodology to the DQN agent. DQN agent become much more complicated to implement if the action space of an environment is continuous. The DDPG agent solves this issue by using 2 neural networks, one (the actor) determines the best action under the current policy, the other (the critic) determines the best action value for the action chosen by the actor. This allows a value to be assigned to an action.\n",
    "\n",
    "### MARL\n",
    "In this project the two agents have seperate, initially identical, actor networks. Each actor network shares the same critic network. The replay buffer, explained below, is also shared. The agents are trained seperately, this results in a wider experience pool to learn form.\n",
    "\n",
    "### Random Replay buffer\n",
    "The Replay Buffer is storage that contains the results of all actions taken. Instead of learning while taking every action the replay buffer provides a list of previously taken actions. At pre determined intervals a number of samples are chosen from the buffer at random this means that the training data is out of sequence. This removes the possibility of certain sequences biasing the training of the neural network. In this porject 10 training loops are performed every 20 timesteps.\n",
    "\n",
    "\n",
    "### Soft Target Update\n",
    "When using soft target updates the changes of the weights in the network are adapted in small steps using the equation below:\n",
    "\n",
    "![](results/softUpdate.png)\n",
    "\n",
    "Research has shown the soft update method has better results than a hard update method, where the steps taken are larger.\n",
    "\n",
    "### Noise\n",
    "In order to produce random varience in the choice of action a noise is introcuced on to the chosen action on each timestep. The noise in this project is generated by Ornstein Uhlenbeck Noise function. The code for this noise function was taken from the open ai baselines repository: https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "The current Hyperparameter list is shown below.\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "\n",
    "BATCH_SIZE = 1024       # minibatch size\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "TAU = 1e-2      # for soft update of target parameters\n",
    "\n",
    "LR_ACTOR = 1e-3        # learning rate of the actor \n",
    "\n",
    "LR_CRITIC = 1e-3     # learning rate of the critic\n",
    "\n",
    "WEIGHT_DECAY = 0.       # weight decay\n",
    "\n",
    "NO_LEARN = 20\n",
    "\n",
    "LEARN_EVERY = 20\n",
    "\n",
    "REPLAY_INIT_STEPS = 1024\n",
    "\n",
    "SEED = 8\n",
    "\n",
    "MU = 0.\n",
    "\n",
    "THETA = 0.15\n",
    "\n",
    "SIGMA = 0.2\n",
    "\n",
    "ACTOR_FC1 = 500\n",
    "\n",
    "ACTOR_FC2 = 500\n",
    "\n",
    "CRITIC_FC1 = 500\n",
    "\n",
    "CRITIC_FC2 = 500\n",
    "\n",
    "N_EPISODES = 1000\n",
    "\n",
    "MAX_T = 1000\n",
    "    \n",
    "# Results\n",
    "The DDPG Agent Developed in this project was able to reach an average score of >0.5 in 642 episodes.\n",
    "\n",
    "![](results/ScoreImage.png)\n",
    "\n",
    "\n",
    "# Future Development\n",
    "There are some known drawbacks with the implementstion in this project due to limitations in the application of the agent. The following processes could improve the performance of the training agent:\n",
    "\n",
    "## Replay Buffer\n",
    "One improvement could be to implement a Prioritised Experience Replay buffer. In the current imlementation all experiences in the replay buffer have equal probability of being chosen. With a priotitised buffer the experiences would be pritoritised in terms of how much the result diverges from the predicted results.\n",
    "\n",
    "\n",
    "## Other Agent Types\n",
    "It may be worth experimenting with other agents such as D4PG, TRPO  and TNPG implementations to investigate the best approach for solving the environment.\n",
    "\n",
    "## Custom rewards\n",
    "As the reward system of this environment only rewards the agent once the episode has finished it may be worth implementing some custom rewards to aid training. The idea behind this approach is to use the environment data timestep by timestep in order to guide the learnng of the agents. for example a small reward could be generated every timestep depending on the proximity of the agent, on the side of the net the ball is, to the ball. This will encourage the agent to track the ball movement in the environment, potentially speeding up the learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
