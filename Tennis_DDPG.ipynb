{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition and Collaboration\n",
    "## Udacity Deep Reinforcement Learning project 3\n",
    "\n",
    "### Import Used Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Graphics Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices Available:\n",
      "0 :  GeForce GTX 970\n",
      "\n",
      "Current Device in use is Device # 0 :  GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "# Check Cuda is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get Device Count\n",
    "    GPUCount = torch.cuda.device_count()\n",
    "    \n",
    "    # Print Device Names\n",
    "    print(\"Devices Available:\")\n",
    "    for i in range(0, GPUCount):\n",
    "        print(i,\": \", torch.cuda.get_device_name(i))\n",
    "\n",
    "    # Check Current Device and Print Explicit Message\n",
    "    print(\"\\nCurrent Device in use is Device #\",\n",
    "          torch.cuda.current_device(),\": \",\n",
    "          torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA Not Available!\\nWill Train on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tennis Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\", no_graphics = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Brain an examine environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Neural Network Definitions\n",
    "### Imports for Neural Network Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Actor Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units = 256, fc2_units = 128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition\n",
    "\n",
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise(object):\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise(ActionNoise):\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = dx\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        \n",
    "        #print(self.state)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, replay_buffer_size, batch_size, seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=replay_buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        exp = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(exp)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([ex.state for ex in experiences if ex is not None])\n",
    "                                 ).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([ex.action for ex in experiences if ex is not None])\n",
    "                                  ).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([ex.reward for ex in experiences if ex is not None])\n",
    "                                  ).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([ex.next_state for ex in experiences if ex is not None])\n",
    "                                      ).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([ex.done for ex in experiences if ex is not None]).astype(np.uint8)\n",
    "                                ).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, \n",
    "                 memory,\n",
    "                 state_size, \n",
    "                 action_size,\n",
    "                 actor_lr,                 \n",
    "                 critic_local,\n",
    "                 critic_target,\n",
    "                 critic_optimiser,\n",
    "                 batch_size,\n",
    "                 n_learn,\n",
    "                 learn_every,\n",
    "                 tau,\n",
    "                 gamma = 0.99, \n",
    "                 fcs1_units=256, \n",
    "                 fc2_units=128, \n",
    "                 seed=0,\n",
    "                 mu=0., \n",
    "                 theta=0.15, \n",
    "                 sigma=0.2,\n",
    "                 decay = 0.\n",
    "                 ):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_learn = n_learn\n",
    "        self.learn_every = learn_every\n",
    "        \n",
    "        self.decay = decay\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # Actor\n",
    "        self.actor_local = Actor(state_size, action_size, seed, fcs1_units, fc2_units).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed, fcs1_units, fc2_units).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),lr=actor_lr)\n",
    "            \n",
    "        self.critic_local = critic_local\n",
    "        self.critic_target = critic_target\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "\n",
    "        # Noise\n",
    "        self.noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_size), sigma = float(0.2) * np.ones(action_size),theta = theta, dt=1e-2)\n",
    "\n",
    "        # Replay Memory\n",
    "        self.memory = memory\n",
    "        \n",
    "    def step(self, time_step):\n",
    "        if time_step % self.learn_every == 0:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                for i in range(self.n_learn):\n",
    "                    experiences = self.memory.sample()\n",
    "                    self.learn(experiences)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (self.gamma  * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        \n",
    "        # Minimize loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target)\n",
    "        self.soft_update(self.actor_local, self.actor_target)\n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(self.tau *local_param.data + (1.0-self.tau )*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill replay buffer with rnd actions\n",
    "\n",
    "def init_replay_buffer(env, memory, steps):\n",
    "    for i in range(1, steps):                                      # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = np.asarray(env_info.rewards)\n",
    "            dones = np.asarray(env_info.local_done)                        # see if episode finished\n",
    "            [memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i]) for i in range(num_agents)]\n",
    "            \n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(agents, memory, n_episodes=500, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores_buffer0 =[]\n",
    "    scores_buffer1 =[]\n",
    "    total_scores = []\n",
    "    train = False;\n",
    "    t = 0\n",
    "    print(\"LR_ACTOR:{:.5f}\\tLR_CRITIC{:.5f}\\tTAU:{:.2f}\\tSEED:{}\".format(LR_ACTOR, LR_CRITIC, TAU, SEED))\n",
    "    init_replay_buffer(env, memory, REPLAY_INIT_STEPS)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]            # reset the environment\n",
    "        [agents[i].reset() for i in range(len(agents))]\n",
    "        states = env_info.vector_observations\n",
    "        scores = np.zeros(num_agents)\n",
    "                \n",
    "        while True:\n",
    "            t = t + 1\n",
    "            \n",
    "            actions = np.array([agents[i].act(states[i], add_noise=True) for i in range(num_agents)])\n",
    "            \n",
    "            env_info = env.step( actions )[brain_name]\n",
    "            \n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            [memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i]) for i in range(num_agents)]\n",
    "            \n",
    "            [agents[i].step(t) for i in range(num_agents)]\n",
    "            \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            score = np.max(scores)\n",
    "            \n",
    "                       \n",
    "            if np.any( dones ): \n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores_buffer0.append(scores[0])\n",
    "        scores_buffer1.append(scores[1]) \n",
    "        \n",
    "        total_scores.append(score)\n",
    "        \n",
    "        print('\\rEpisode {}\\tScore0: {:.2f}\\tScore1: {:.2f}\\tMax:{:.2f}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_buffer0[len(scores_buffer0) - 1]) , np.mean(scores_buffer1[len(scores_buffer1) - 1]), score ,np.mean(scores_deque)), end=\"\")\n",
    "        for i in range(num_agents):\n",
    "            torch.save(agents[i].actor_local.state_dict(), 'checkpoint_actor{}.pth'.format(i))\n",
    "        torch.save(critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            #tau = tau+0.1\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}                                      '.format(i_episode, np.mean(scores_deque)))\n",
    "        if(np.mean(scores_deque) > 0.5):\n",
    "            break\n",
    "            \n",
    "    return scores_buffer0, scores_buffer1, total_scores, scores_deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 1024       # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-2      # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3        # learning rate of the actor \n",
    "LR_CRITIC = 1e-3     # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.       # weight decay\n",
    "NO_LEARN = 20\n",
    "LEARN_EVERY = 20\n",
    "REPLAY_INIT_STEPS = 1024\n",
    "SEED = 8\n",
    "\n",
    "MU = 0.\n",
    "THETA = 0.15\n",
    "SIGMA = 0.2\n",
    "\n",
    "ACTOR_FC1 = 500\n",
    "ACTOR_FC2 = 500\n",
    "\n",
    "CRITIC_FC1 = 500\n",
    "CRITIC_FC2 = 500\n",
    "\n",
    "N_EPISODES = 1000\n",
    "\n",
    "MAX_T = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent and run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_local = Critic(state_size, action_size, SEED, CRITIC_FC1, CRITIC_FC2).to(device)\n",
    "critic_target = Critic(state_size, action_size, SEED, CRITIC_FC1, CRITIC_FC2).to(device)\n",
    "critic_optimizer = optim.Adam(critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, SEED)\n",
    "\n",
    "Agents = np.array([Agent(memory,\n",
    "                         state_size,\n",
    "                         action_size, \n",
    "                         LR_ACTOR,                 \n",
    "                         critic_local,\n",
    "                         critic_target,\n",
    "                         critic_optimizer,\n",
    "                         BATCH_SIZE,\n",
    "                         NO_LEARN,\n",
    "                         LEARN_EVERY,\n",
    "                         TAU,\n",
    "                         GAMMA, \n",
    "                         ACTOR_FC1, \n",
    "                         ACTOR_FC2, \n",
    "                         SEED,\n",
    "                         MU, \n",
    "                         THETA, \n",
    "                         SIGMA,\n",
    "                         WEIGHT_DECAY\n",
    "                         ) for i in range(num_agents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_ACTOR:0.00100\tLR_CRITIC0.00100\tTAU:0.01\tSEED:8\n",
      "Episode 100\tAverage Score: 0.00                                      \n",
      "Episode 200\tAverage Score: 0.04                                      \n",
      "Episode 300\tAverage Score: 0.04                                      \n",
      "Episode 400\tAverage Score: 0.09                                      \n",
      "Episode 500\tAverage Score: 0.12                                      \n",
      "Episode 600\tAverage Score: 0.29                                      \n",
      "Episode 643\tScore0: 0.80\tScore1: 0.79\tMax:0.80\tAverage Score: 0.508"
     ]
    }
   ],
   "source": [
    "scores0, scores1, combined, average100 = ddpg(Agents, memory, N_EPISODES, MAX_T, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Solved at Episode:  642\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "scores_average = []\n",
    "pass_episode = 0\n",
    "\n",
    "average_over = 100\n",
    "average_buffer = deque(maxlen=100)\n",
    "running_score_history = []\n",
    "\n",
    "for i in range(len(combined)):\n",
    "    average_buffer.append(combined[i])\n",
    "    scores_average.append(np.mean(average_buffer));\n",
    "    if(scores_average[i] >= 0.5 and pass_episode == 0):\n",
    "        pass_episode = i;\n",
    "    \n",
    "ax = fig.add_subplot(111)\n",
    "#plt.plot(np.arange(1, len(scores_average0)+1), scores_average0, 'b')\n",
    "#plt.plot(np.arange(1, len(scores_average1)+1), scores_average1, 'g')\n",
    "plt.plot(np.arange(1, len(scores_average)+1), scores_average, 'g')\n",
    "\n",
    "plt.axhline(0.5,0,150, color='r', linewidth=2.0 ,alpha=0.4 )\n",
    "plt.axvline(pass_episode,0,40, color='r', linewidth=2.0, alpha=0.4 )\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "print(\"0 Solved at Episode: \", pass_episode);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
